# Comparison of language models in natural language generation tasks

Copyright (c) 2018 Christian Westbrook

### Abstract

### Introduction

When designing a system that requires the processing of natural language it is
usually necessary to represent this natural language in some unnatural format
that is more useful for computation. Natural language is frequently restructured
statistically into language models that represent the distribution of
probabilities across sequences of words within a given corpus. The particular
model implemented has a dramatic impact on the overall performance of a natural
language processing system. In this we compare the performance of an n-gram
language model and a neural language model given the task of natural language
generation across a corpus of blog posts collected from blogger.com.

### Background

### Specification

### Implementation

### Evaluation

### Conclusions

### Data

The Blog Authorship Corpus http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm

### References

J. Schler, M. Koppel, S. Argamon and J. Pennebaker (2006). Effects of Age and
Gender on Blogging in *Proceedings of 2006 AAAI Spring Symposium on
Computational Approaches for Analyzing Weblogs*.
